{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "Simple neural netowrk with self-defined logistic regression, linear regression, regularized logistic regression, regularized linear regression, sigmoid function, ReLU function, and softmax function.\n",
    "\n",
    "### Dataset Analysis:\n",
    "the dataset includes cat, dog and other species. additionally, an image may contains two species. \n",
    "* we should first do face detection from line segments, to segments grouping in order to form a small region or part of the face, to forming an entire face by grouping based on the regions. in the following layer, the model should be able to identify if the face is one of cat, dog, or wild animals. in this part, there will be at least 3 layers given there will be at least one for line segments, at least one for segment grouping into region, amd at least one for face forming based on regions.\n",
    "* in this case, in certain step we shoudl identify if the picture contains cat, dog, and e.g. rabbit or no. so the here it should be binary classification for each unit(neuron). e.g. only two classes, cat and dog. output y = [ bool_has_cat, bool_has_dog ]. Hence this layer should be for instance using sigmoid as activation function.\n",
    "* Then, in the following layer, emotion should be identified to learn that y = [cat happy?, dog happy?, cat sad?, dog sad?, cat angry?, dog angry?, cat relax?, dog relax?] which given each emotional class we should be answering yes or no. if the image is without cat, then all emotional classes relating to cat should be zero. if the image contains cat and dog, emotional classes relating cat and dog should at least have two ones such that cat has certain emotoin relating to a emotion category, and dog should also have certain emotion relating to certain category. it is possible that the specie is happy and sad at the same time such that there are more than two ones. It is due to this that for each layer, it should for instance use sigmoid activatoin function.\n",
    "* given that the chance of having that emotion category is approximated, now this follwoing layer should further learn the level of emotion for each emotional class. for instance, if the image contains cat and dog and that for cat the happy and relaxed are both detected, the two neurons for cat happy and cat relaxed should one compute the level of happy and the other one compute the level of relaxed. the same logic applies to dog. in this case, the function should be ReLU. e.g. y = [level of cat is happy, level of dog is happy, level of cat is sad, level of dog is sad, level of cat is angry, level of dog is angry, level of cat is relaxed, level of dog is relaxed]. if the image contains only cat, then all emtional classes level relating to dog should be zero.\n",
    "* now this layer should identify based on the level of each emotional class. for instance, let's say, the input to this layer is = [level of cat is happy, level of dog is happy, level of cat is sad, level of dog is sad, level of cat is angry, level of dog is angry, level of cat is relaxed, level of dog is relaxed] and that the image contains both cat and dog. cat is detected as happy and relaxed, dog is detected as happy and sad, the input to this layer is then = [0.5, 0.4, 0.0, 0.5, 0.0, 0.0, 0.7, 0.0], then the output of this layer should contains only two ones being [0, 0, 0, 1, 0, 0, 1, 0] such that the cat is relaxed and dog is sad. this layer hence should use either softmax, or, for each unit, for instance, a sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.metrics import CategoricalAccuracy\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel\n",
    "tf.autograph.set_verbosity(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data pre-processing\n",
    "* Since in the baseline model insteresed implementation should use conventional model, we'll stick with simple neural networks from draft for educational purposes, although it's important to note that this approach will possibily yield a very poor result for such a complex image processing task.\n",
    "* First, you need to preprocess your images. This involves loading the images, resizing them to a uniform size, converting them to grayscale, and flattening them into vectors.\n",
    "\n",
    "#### Benefits of Using Grayscale Images\n",
    "Reduced Complexity: Grayscale images are less complex than color images, making them easier to process with simpler algorithms.\n",
    "Reduced Computational Load: Grayscale images require less computational power and memory, as they have only one channel compared to three in color images.\n",
    "Focus on Texture and Shape: Converting to grayscale can help the model focus on the texture and shape information, which might be more relevant for certain tasks like emotion detection in animals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess images\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            img = cv2.resize(img, (128, 128))  # Resize to a fixed size for the model\n",
    "            images.append(img)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Labeling the Data\n",
    "If one has dataset for training and is without labels, one will need to assign labels to the data. Since we have separate folders for each emotion and labels for each data, we can skipp this part.\n",
    "#### Define the path to the sub dataset folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the dataset folders\n",
    "happy_folder = \"pets_facial_expression_dataset/happy\"\n",
    "sad_folder = \"pets_facial_expression_dataset/Sad\"\n",
    "angry_folder = \"pets_facial_expression_dataset/Angry\"\n",
    "other_folder = \"pets_facial_expression_dataset/Other\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data and Combine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images and labels for each emotion\n",
    "happy_images = load_images_from_folder(happy_folder)\n",
    "sad_images = load_images_from_folder(sad_folder)\n",
    "angry_images = load_images_from_folder(angry_folder)\n",
    "other_images = load_images_from_folder(other_folder)\n",
    "\n",
    "\n",
    "# Create labels for each emotion category\n",
    "happy_labels = [0] * len(happy_images)\n",
    "sad_labels = [1] * len(sad_images)\n",
    "angry_labels = [2] * len(angry_images)\n",
    "other_labels = [3] * len(other_folder)\n",
    "\n",
    "\n",
    "# Concatenate images and labels\n",
    "X = np.array(happy_images + sad_images + angry_images + other_images)\n",
    "y = np.array(happy_labels + sad_labels + angry_labels + other_labels)\n",
    "\n",
    "# Normalize pixel values to range [0, 1]\n",
    "X = X.astype('float32') / 255.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "y = to_categorical(y, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Splitting the Data\n",
    "Split dataset into training and testing sets. This is essential for evaluating the performance of the model.\n",
    "A common split is 80% for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define the Network Architecture\n",
    "1. Face Detection and Segmentation (line, segments, face)\n",
    "* **Objective**: Detect and segment animal faces from images.\n",
    "* **Approach**: This typically requires convolutional neural networks (CNNs) that can identify patterns (like edges, textures) and group them into larger structures (like faces).\n",
    "* **Layers**: Start with convolutional layers for feature extraction, followed by pooling layers to reduce dimensionality, and fully connected layers for classification.\n",
    "Activation Functions: ReLU is commonly used in CNNs for its efficiency.\n",
    "2. Species Identification (if we have the labels)\n",
    "* **Objective**: Identify whether the image contains a cat, dog, or other species.\n",
    "* **Approach**: This is a multi-label classification problem (since an image can have more than one label).\n",
    "* **Layers**: Fully connected layers following the feature extraction layers.\n",
    "* **Activation Function**:** Sigmoid activation function for each neuron (since it's a binary classification for each species).\n",
    "3. Final Layer for Emotion Classification\n",
    "* **Objective**: Classify the predominant emotion for each species.\n",
    "* **Approach**: This is a classification problem, but with a twist. You're interested in the predominant emotion, which is a bit different from standard classification.\n",
    "* **Layers**: Fully connected layer.\n",
    "* **Activation Function**: Softmax if you're classifying one predominant emotion per species, or sigmoid for binary classification of each emotion.\n",
    "\n",
    "### Additional Considerations:\n",
    "* **Data Preprocessing**: Ensure images are properly preprocessed (normalized, resized, etc.).\n",
    "* **Model Complexity**: This is a complex model. Start with a simpler version and iteratively add complexity.\n",
    "* **Training Data**: You'll need a large and well-labeled dataset for this task, especially for the emotion detection and intensity levels.\n",
    "* **Evaluation Metrics**: Choose appropriate metrics for each stage (accuracy, F1 score, mean squared error for intensity levels, etc.).\n",
    "* **Computational Resources**: This model might require significant computational resources, especially for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Configure the NN\n",
    "Since we have four output classes (happy, sad, angry, relaxed), the last output layer should have 4 neurons with a softmax activation function for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the simple neural netowrk model\n",
    "\n",
    "# emotion_model = Sequential(\n",
    "#     [               \n",
    "#         Flatten(input_shape=(128, 128)),  # The input shape is the size of the images,    #specify input size\n",
    "#         ### START CODE HERE ### \n",
    "#         Dense(512, activation=\"relu\", name=\"line\"),\n",
    "#         Dense(256, activation=\"relu\", name=\"segment\"),\n",
    "#         Dense(128, activation=\"relu\", name=\"face\"),\n",
    "#         Dense(3, activation=\"relu\", name=\"species\"),\n",
    "#         Dense(3, activation=\"softmax\", name=\"emotion\"),\n",
    "#         ### END CODE HERE ### \n",
    "#     ], name = \"pet_emotion\" \n",
    "# )\n",
    "\n",
    "# [line, segment, face, species, emotion] = emotion_model.layers[1:]\n",
    "# #### Examine Weights shapes\n",
    "# W1,b1 = line.get_weights()\n",
    "# W2,b2 = segment.get_weights()\n",
    "# W3,b3 = face.get_weights()\n",
    "# W4,b4 = species.get_weights()\n",
    "# W5,b5 = emotion.get_weights()\n",
    "\n",
    "# print(f\"W1 shape = {W1.shape}, b1 shape = {b1.shape}\")\n",
    "# print(f\"W2 shape = {W2.shape}, b2 shape = {b2.shape}\")\n",
    "# print(f\"W3 shape = {W3.shape}, b3 shape = {b3.shape}\")\n",
    "# print(f\"W4 shape = {W4.shape}, b4 shape = {b4.shape}\")\n",
    "# print(f\"W5 shape = {W5.shape}, b5 shape = {b5.shape}\")\n",
    "# # Compile the model\n",
    "# emotion_model.compile(\n",
    "#     loss=CategoricalCrossentropy(from_logits=False),\n",
    "#     optimizer=Adam(learning_rate=0.001),\n",
    "#     metrics=[CategoricalAccuracy()]\n",
    "# )\n",
    "\n",
    "# Define the model\n",
    "emotion_model = Sequential([\n",
    "    Flatten(input_shape=(128, 128)),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(3, activation='softmax')  # Output layer for 3 classes with softmax activation\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "emotion_model.compile(\n",
    "    loss=CategoricalCrossentropy(from_logits=False),\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    metrics=[CategoricalAccuracy()]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine details of the model by first extracting the layers with `model.layers` and then extracting the weights with `layerx.get_weights()` as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Train the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 [==============================] - 2s 32ms/step - loss: 1.3935 - categorical_accuracy: 0.3519 - val_loss: 1.2150 - val_categorical_accuracy: 0.4167\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 0s 20ms/step - loss: 1.1270 - categorical_accuracy: 0.3648 - val_loss: 1.0653 - val_categorical_accuracy: 0.4000\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 0s 20ms/step - loss: 1.0943 - categorical_accuracy: 0.4148 - val_loss: 1.0771 - val_categorical_accuracy: 0.4667\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 0s 19ms/step - loss: 1.0616 - categorical_accuracy: 0.4463 - val_loss: 1.1081 - val_categorical_accuracy: 0.4333\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 0s 20ms/step - loss: 1.0562 - categorical_accuracy: 0.4815 - val_loss: 1.0672 - val_categorical_accuracy: 0.5167\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 0s 19ms/step - loss: 1.0229 - categorical_accuracy: 0.5074 - val_loss: 1.1749 - val_categorical_accuracy: 0.3833\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 0s 19ms/step - loss: 1.0365 - categorical_accuracy: 0.5019 - val_loss: 1.0655 - val_categorical_accuracy: 0.5000\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.9864 - categorical_accuracy: 0.5370 - val_loss: 1.1025 - val_categorical_accuracy: 0.4833\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.9648 - categorical_accuracy: 0.5315 - val_loss: 1.2333 - val_categorical_accuracy: 0.4000\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.9947 - categorical_accuracy: 0.5370 - val_loss: 1.0857 - val_categorical_accuracy: 0.4500\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 1.0053 - categorical_accuracy: 0.5315 - val_loss: 1.2092 - val_categorical_accuracy: 0.4000\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.9594 - categorical_accuracy: 0.5481 - val_loss: 1.1151 - val_categorical_accuracy: 0.4667\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.9459 - categorical_accuracy: 0.5685 - val_loss: 1.2078 - val_categorical_accuracy: 0.4167\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 0s 19ms/step - loss: 0.9104 - categorical_accuracy: 0.5796 - val_loss: 1.1732 - val_categorical_accuracy: 0.3667\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 0s 29ms/step - loss: 0.9555 - categorical_accuracy: 0.5574 - val_loss: 1.0782 - val_categorical_accuracy: 0.4833\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - 0s 19ms/step - loss: 0.8621 - categorical_accuracy: 0.6296 - val_loss: 1.0876 - val_categorical_accuracy: 0.4500\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - 0s 20ms/step - loss: 0.8076 - categorical_accuracy: 0.6481 - val_loss: 1.2233 - val_categorical_accuracy: 0.4333\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - 0s 20ms/step - loss: 0.8174 - categorical_accuracy: 0.6611 - val_loss: 1.1974 - val_categorical_accuracy: 0.4167\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - 0s 19ms/step - loss: 0.8998 - categorical_accuracy: 0.5926 - val_loss: 1.2584 - val_categorical_accuracy: 0.4333\n",
      "Epoch 20/100\n",
      "17/17 [==============================] - 0s 19ms/step - loss: 0.8150 - categorical_accuracy: 0.6593 - val_loss: 1.1506 - val_categorical_accuracy: 0.4500\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.7899 - categorical_accuracy: 0.6611 - val_loss: 1.1487 - val_categorical_accuracy: 0.4667\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.7809 - categorical_accuracy: 0.6611 - val_loss: 1.2967 - val_categorical_accuracy: 0.4000\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.6843 - categorical_accuracy: 0.7222 - val_loss: 1.3011 - val_categorical_accuracy: 0.4167\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.6496 - categorical_accuracy: 0.7444 - val_loss: 1.2891 - val_categorical_accuracy: 0.4167\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.6031 - categorical_accuracy: 0.7556 - val_loss: 1.3294 - val_categorical_accuracy: 0.4500\n",
      "Epoch 26/100\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.5408 - categorical_accuracy: 0.7963 - val_loss: 1.6284 - val_categorical_accuracy: 0.4000\n",
      "Epoch 27/100\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.5690 - categorical_accuracy: 0.7722 - val_loss: 1.4558 - val_categorical_accuracy: 0.4167\n",
      "Epoch 28/100\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.5791 - categorical_accuracy: 0.7611 - val_loss: 1.3624 - val_categorical_accuracy: 0.4833\n",
      "Epoch 29/100\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.4465 - categorical_accuracy: 0.8370 - val_loss: 1.7635 - val_categorical_accuracy: 0.5167\n",
      "Epoch 30/100\n",
      "17/17 [==============================] - 0s 19ms/step - loss: 0.4751 - categorical_accuracy: 0.8222 - val_loss: 1.7085 - val_categorical_accuracy: 0.5000\n",
      "Epoch 31/100\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.5699 - categorical_accuracy: 0.7685 - val_loss: 1.5449 - val_categorical_accuracy: 0.4500\n",
      "Epoch 32/100\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.5364 - categorical_accuracy: 0.7944 - val_loss: 1.8533 - val_categorical_accuracy: 0.4000\n",
      "Epoch 33/100\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.4760 - categorical_accuracy: 0.8315 - val_loss: 1.6253 - val_categorical_accuracy: 0.4500\n",
      "Epoch 34/100\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.4166 - categorical_accuracy: 0.8315 - val_loss: 1.8623 - val_categorical_accuracy: 0.4500\n",
      "Epoch 35/100\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.3037 - categorical_accuracy: 0.8722 - val_loss: 2.1038 - val_categorical_accuracy: 0.4667\n",
      "Epoch 36/100\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.5113 - categorical_accuracy: 0.8000 - val_loss: 1.9895 - val_categorical_accuracy: 0.4333\n",
      "Epoch 37/100\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.5516 - categorical_accuracy: 0.7685 - val_loss: 1.3787 - val_categorical_accuracy: 0.5333\n",
      "Epoch 38/100\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.4244 - categorical_accuracy: 0.8407 - val_loss: 1.6979 - val_categorical_accuracy: 0.5333\n",
      "Epoch 39/100\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.2752 - categorical_accuracy: 0.9167 - val_loss: 1.7907 - val_categorical_accuracy: 0.4500\n",
      "Epoch 40/100\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.2291 - categorical_accuracy: 0.9241 - val_loss: 2.4081 - val_categorical_accuracy: 0.4500\n",
      "Epoch 41/100\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.3311 - categorical_accuracy: 0.8759 - val_loss: 2.0357 - val_categorical_accuracy: 0.4667\n",
      "Epoch 42/100\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.3416 - categorical_accuracy: 0.8630 - val_loss: 1.8523 - val_categorical_accuracy: 0.5000\n",
      "Epoch 43/100\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.4208 - categorical_accuracy: 0.8241 - val_loss: 1.6823 - val_categorical_accuracy: 0.4833\n",
      "Epoch 44/100\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.1880 - categorical_accuracy: 0.9389 - val_loss: 2.1547 - val_categorical_accuracy: 0.5000\n",
      "Epoch 45/100\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.2001 - categorical_accuracy: 0.9352 - val_loss: 2.3790 - val_categorical_accuracy: 0.5000\n",
      "Epoch 46/100\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.2465 - categorical_accuracy: 0.9000 - val_loss: 2.0801 - val_categorical_accuracy: 0.5000\n",
      "Epoch 47/100\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.1650 - categorical_accuracy: 0.9407 - val_loss: 2.2951 - val_categorical_accuracy: 0.4667\n",
      "Epoch 48/100\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.1149 - categorical_accuracy: 0.9648 - val_loss: 2.3788 - val_categorical_accuracy: 0.4500\n",
      "Epoch 49/100\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.1254 - categorical_accuracy: 0.9537 - val_loss: 3.3737 - val_categorical_accuracy: 0.4167\n",
      "Epoch 50/100\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.4034 - categorical_accuracy: 0.8667 - val_loss: 2.0577 - val_categorical_accuracy: 0.3833\n",
      "Epoch 51/100\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.2067 - categorical_accuracy: 0.9222 - val_loss: 2.2131 - val_categorical_accuracy: 0.4500\n",
      "Epoch 52/100\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.1794 - categorical_accuracy: 0.9352 - val_loss: 2.1920 - val_categorical_accuracy: 0.4833\n",
      "Epoch 53/100\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.1960 - categorical_accuracy: 0.9296 - val_loss: 1.9372 - val_categorical_accuracy: 0.4500\n",
      "Epoch 54/100\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.1155 - categorical_accuracy: 0.9667 - val_loss: 2.3308 - val_categorical_accuracy: 0.4167\n",
      "Epoch 55/100\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.0638 - categorical_accuracy: 0.9833 - val_loss: 2.7099 - val_categorical_accuracy: 0.4000\n",
      "Epoch 56/100\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.1522 - categorical_accuracy: 0.9537 - val_loss: 2.5151 - val_categorical_accuracy: 0.4167\n",
      "Epoch 57/100\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.1700 - categorical_accuracy: 0.9259 - val_loss: 2.8489 - val_categorical_accuracy: 0.4833\n",
      "Epoch 58/100\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.1331 - categorical_accuracy: 0.9481 - val_loss: 3.0603 - val_categorical_accuracy: 0.4167\n",
      "Epoch 59/100\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.0738 - categorical_accuracy: 0.9741 - val_loss: 3.0474 - val_categorical_accuracy: 0.4667\n",
      "Epoch 60/100\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.0397 - categorical_accuracy: 0.9907 - val_loss: 3.1366 - val_categorical_accuracy: 0.4500\n",
      "Epoch 61/100\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.1971 - categorical_accuracy: 0.9296 - val_loss: 2.9242 - val_categorical_accuracy: 0.5000\n",
      "Epoch 62/100\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.2005 - categorical_accuracy: 0.9352 - val_loss: 3.0753 - val_categorical_accuracy: 0.4667\n",
      "Epoch 63/100\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 0.2696 - categorical_accuracy: 0.9019 - val_loss: 2.4003 - val_categorical_accuracy: 0.5000\n",
      "Epoch 64/100\n",
      "17/17 [==============================] - 0s 20ms/step - loss: 0.1065 - categorical_accuracy: 0.9630 - val_loss: 2.9147 - val_categorical_accuracy: 0.4500\n",
      "Epoch 65/100\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.0306 - categorical_accuracy: 0.9963 - val_loss: 3.1766 - val_categorical_accuracy: 0.4333\n",
      "Epoch 66/100\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.0137 - categorical_accuracy: 1.0000 - val_loss: 3.3655 - val_categorical_accuracy: 0.4333\n",
      "Epoch 67/100\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.0072 - categorical_accuracy: 1.0000 - val_loss: 3.5890 - val_categorical_accuracy: 0.4167\n",
      "Epoch 68/100\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.0067 - categorical_accuracy: 1.0000 - val_loss: 3.6912 - val_categorical_accuracy: 0.4167\n",
      "Epoch 69/100\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.0054 - categorical_accuracy: 1.0000 - val_loss: 3.6960 - val_categorical_accuracy: 0.3833\n",
      "Epoch 70/100\n",
      "17/17 [==============================] - 0s 20ms/step - loss: 0.0056 - categorical_accuracy: 1.0000 - val_loss: 3.7328 - val_categorical_accuracy: 0.4000\n",
      "Epoch 71/100\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.0063 - categorical_accuracy: 1.0000 - val_loss: 3.6987 - val_categorical_accuracy: 0.4167\n",
      "Epoch 72/100\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 0.0109 - categorical_accuracy: 0.9981 - val_loss: 4.0637 - val_categorical_accuracy: 0.4333\n",
      "Epoch 73/100\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.0106 - categorical_accuracy: 0.9981 - val_loss: 3.8250 - val_categorical_accuracy: 0.4000\n",
      "Epoch 74/100\n",
      "17/17 [==============================] - 0s 20ms/step - loss: 0.0204 - categorical_accuracy: 0.9963 - val_loss: 3.9550 - val_categorical_accuracy: 0.4833\n",
      "Epoch 75/100\n",
      "17/17 [==============================] - 0s 19ms/step - loss: 0.1683 - categorical_accuracy: 0.9426 - val_loss: 4.1565 - val_categorical_accuracy: 0.4667\n",
      "Epoch 76/100\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.3881 - categorical_accuracy: 0.8667 - val_loss: 2.9051 - val_categorical_accuracy: 0.3500\n",
      "Epoch 77/100\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.2380 - categorical_accuracy: 0.9056 - val_loss: 2.7298 - val_categorical_accuracy: 0.4500\n",
      "Epoch 78/100\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.1005 - categorical_accuracy: 0.9667 - val_loss: 3.4512 - val_categorical_accuracy: 0.4333\n",
      "Epoch 79/100\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.2020 - categorical_accuracy: 0.9148 - val_loss: 3.0069 - val_categorical_accuracy: 0.4833\n",
      "Epoch 80/100\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.2397 - categorical_accuracy: 0.9074 - val_loss: 2.8602 - val_categorical_accuracy: 0.4167\n",
      "Epoch 81/100\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.2316 - categorical_accuracy: 0.9167 - val_loss: 2.2780 - val_categorical_accuracy: 0.4833\n",
      "Epoch 82/100\n",
      "17/17 [==============================] - 0s 19ms/step - loss: 0.0886 - categorical_accuracy: 0.9796 - val_loss: 2.8355 - val_categorical_accuracy: 0.5000\n",
      "Epoch 83/100\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.0444 - categorical_accuracy: 0.9907 - val_loss: 2.9649 - val_categorical_accuracy: 0.4833\n",
      "Epoch 84/100\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.3354 - categorical_accuracy: 0.9074 - val_loss: 3.2566 - val_categorical_accuracy: 0.4500\n",
      "Epoch 85/100\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.2686 - categorical_accuracy: 0.8944 - val_loss: 2.3381 - val_categorical_accuracy: 0.4833\n",
      "Epoch 86/100\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 0.1537 - categorical_accuracy: 0.9519 - val_loss: 2.9199 - val_categorical_accuracy: 0.4167\n",
      "Epoch 87/100\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 0.1098 - categorical_accuracy: 0.9630 - val_loss: 3.1070 - val_categorical_accuracy: 0.4500\n",
      "Epoch 88/100\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.1251 - categorical_accuracy: 0.9648 - val_loss: 2.7304 - val_categorical_accuracy: 0.5000\n",
      "Epoch 89/100\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.1014 - categorical_accuracy: 0.9593 - val_loss: 3.8671 - val_categorical_accuracy: 0.4167\n",
      "Epoch 90/100\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.4170 - categorical_accuracy: 0.8519 - val_loss: 2.7856 - val_categorical_accuracy: 0.4333\n",
      "Epoch 91/100\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.3125 - categorical_accuracy: 0.8815 - val_loss: 2.2992 - val_categorical_accuracy: 0.4500\n",
      "Epoch 92/100\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.1361 - categorical_accuracy: 0.9667 - val_loss: 2.4193 - val_categorical_accuracy: 0.4333\n",
      "Epoch 93/100\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.0430 - categorical_accuracy: 0.9907 - val_loss: 2.9685 - val_categorical_accuracy: 0.4333\n",
      "Epoch 94/100\n",
      "17/17 [==============================] - 0s 19ms/step - loss: 0.0173 - categorical_accuracy: 0.9981 - val_loss: 3.3519 - val_categorical_accuracy: 0.4500\n",
      "Epoch 95/100\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.0096 - categorical_accuracy: 1.0000 - val_loss: 3.5608 - val_categorical_accuracy: 0.4667\n",
      "Epoch 96/100\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 0.0061 - categorical_accuracy: 1.0000 - val_loss: 3.7061 - val_categorical_accuracy: 0.4333\n",
      "Epoch 97/100\n",
      "17/17 [==============================] - 0s 19ms/step - loss: 0.0034 - categorical_accuracy: 1.0000 - val_loss: 3.9181 - val_categorical_accuracy: 0.4500\n",
      "Epoch 98/100\n",
      "17/17 [==============================] - 0s 30ms/step - loss: 0.0026 - categorical_accuracy: 1.0000 - val_loss: 3.9890 - val_categorical_accuracy: 0.4667\n",
      "Epoch 99/100\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.0020 - categorical_accuracy: 1.0000 - val_loss: 4.1273 - val_categorical_accuracy: 0.4500\n",
      "Epoch 100/100\n",
      "17/17 [==============================] - 0s 19ms/step - loss: 0.0017 - categorical_accuracy: 1.0000 - val_loss: 4.2687 - val_categorical_accuracy: 0.4500\n",
      "Training accuracy: 100.00%\n",
      "Validation accuracy: 45.00%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = emotion_model.fit(X_train, y_train, epochs=100, validation_split=0.1)  # 128 to 0.1874, 100 to 0.05\n",
    "\n",
    "# For training accuracy and loss\n",
    "training_accuracy = history.history['categorical_accuracy']\n",
    "training_loss = history.history['loss']\n",
    "\n",
    "# For validation accuracy and loss\n",
    "validation_accuracy = history.history['val_categorical_accuracy']\n",
    "validation_loss = history.history['val_loss']\n",
    "\n",
    "# print the training and validation accuracy percentage\n",
    "print(f\"Training accuracy: {training_accuracy[-1]*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {validation_accuracy[-1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict on test data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, 21)  # number of epochs = 1000\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, training_accuracy, 'b-', label='Training accuracy')\n",
    "plt.plot(epochs, validation_accuracy, 'r-', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, training_loss, 'b-', label='Training loss')\n",
    "plt.plot(epochs, validation_loss, 'r-', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.save('base_facial_emotion_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Test Angry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "# Load the saved model\n",
    "loaded_model = load_model(\"facial_expression_model.h5\")\n",
    "\n",
    "\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            img = cv2.resize(img, (128, 48))  # Resize to a fixed size for the model\n",
    "            images.append(img)\n",
    "    return images\n",
    "\n",
    "\n",
    "# Load a custom test image\n",
    "custom_test_image_path = \"pets_facial_expression_dataset/Angry/35.jpg\"\n",
    "\n",
    "custom_test_image = cv2.imread(custom_test_image_path)\n",
    "custom_test_image = cv2.cvtColor(custom_test_image, cv2.COLOR_BGR2GRAY)\n",
    "custom_test_image = cv2.resize(custom_test_image, (128, 128))\n",
    "custom_test_image = custom_test_image.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the image to match the model input shape\n",
    "custom_test_image = np.expand_dims(custom_test_image, axis=0)\n",
    "custom_test_image = np.expand_dims(custom_test_image, axis=-1)\n",
    "\n",
    "# Make predictions on the custom test image\n",
    "prediction = loaded_model.predict(custom_test_image)\n",
    "prediction_prob = prediction[0]\n",
    "\n",
    "emotion_label = np.argmax(prediction[0])\n",
    "\n",
    "# Map the predicted label to emotion class\n",
    "emotion_classes = {0: 'happy', 1: 'sad', 2: 'angry'}\n",
    "predicted_emotion = emotion_classes[emotion_label]\n",
    "\n",
    "# Print the custom test image and its predicted label\n",
    "print(f\"Predicted Emotion: {predicted_emotion}\")\n",
    "print(f\"Confidence [happy, sad, angry]: {prediction_prob}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Display the custom test image using matplotlib\n",
    "plt.imshow(custom_test_image[0, :, :, 0])\n",
    "plt.title(f\"Predicted Emotion: {predicted_emotion}\")\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()\n",
    "\n",
    "from PIL import Image\n",
    "# Display the original custom test image using PIL\n",
    "img_pil = Image.open(custom_test_image_path)\n",
    "plt.imshow(np.array(img_pil))\n",
    "plt.title(f\"Predicted Emotion: {predicted_emotion}\")\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Test Happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "# Load the saved model\n",
    "loaded_model = load_model(\"facial_expression_model.h5\")\n",
    "\n",
    "\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            img = cv2.resize(img, (128, 128))  # Resize to a fixed size for the model\n",
    "            images.append(img)\n",
    "    return images\n",
    "\n",
    "\n",
    "# Load a custom test image\n",
    "custom_test_image_path = \"pets_facial_expression_dataset/happy/003.jpg\"\n",
    "\n",
    "custom_test_image = cv2.imread(custom_test_image_path)\n",
    "custom_test_image = cv2.cvtColor(custom_test_image, cv2.COLOR_BGR2GRAY)\n",
    "custom_test_image = cv2.resize(custom_test_image, (128, 128))\n",
    "custom_test_image = custom_test_image.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the image to match the model input shape\n",
    "custom_test_image = np.expand_dims(custom_test_image, axis=0)\n",
    "custom_test_image = np.expand_dims(custom_test_image, axis=-1)\n",
    "\n",
    "# Make predictions on the custom test image\n",
    "prediction = loaded_model.predict(custom_test_image)\n",
    "prediction_prob = prediction[0]\n",
    "\n",
    "emotion_label = np.argmax(prediction[0])\n",
    "\n",
    "# Map the predicted label to emotion class\n",
    "emotion_classes = {0: 'happy', 1: 'sad', 2: 'angry'}\n",
    "predicted_emotion = emotion_classes[emotion_label]\n",
    "\n",
    "# Print the custom test image and its predicted label\n",
    "print(f\"Predicted Emotion: {predicted_emotion}\")\n",
    "print(f\"Confidence [happy, sad, angry]: {prediction_prob}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display the custom test image using matplotlib\n",
    "plt.imshow(custom_test_image[0, :, :, 0])\n",
    "plt.title(f\"Predicted Emotion: {predicted_emotion}\")\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()\n",
    "\n",
    "from PIL import Image\n",
    "# Display the original custom test image using PIL\n",
    "img_pil = Image.open(custom_test_image_path)\n",
    "plt.imshow(np.array(img_pil))\n",
    "plt.title(f\"Predicted Emotion: {predicted_emotion}\")\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Test Sad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aerts\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\aerts\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\aerts\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\aerts\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\aerts\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\aerts\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\aerts\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 48, 48), found shape=(None, 128, 128, 1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26040/910650931.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# Make predictions on the custom test image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_test_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[0mprediction_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\u001b[0m in \u001b[0;36mtf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\aerts\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\aerts\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\aerts\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\aerts\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\aerts\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\aerts\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 48, 48), found shape=(None, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "# Load the saved model\n",
    "loaded_model = load_model(\"base_facial_emotion_model.h5\")\n",
    "\n",
    "\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            img = cv2.resize(img, (128, 128))  # Resize to a fixed size for the model\n",
    "            images.append(img)\n",
    "    return images\n",
    "\n",
    "\n",
    "# Load a custom test image\n",
    "custom_test_image_path = \"pets_facial_expression_dataset/Sad/001.jpg\"\n",
    "\n",
    "custom_test_image = cv2.imread(custom_test_image_path)\n",
    "custom_test_image = cv2.cvtColor(custom_test_image, cv2.COLOR_BGR2GRAY)\n",
    "custom_test_image = cv2.resize(custom_test_image, (128, 128))\n",
    "custom_test_image = custom_test_image.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the image to match the model input shape\n",
    "custom_test_image = np.expand_dims(custom_test_image, axis=0)\n",
    "custom_test_image = np.expand_dims(custom_test_image, axis=-1)\n",
    "\n",
    "# Make predictions on the custom test image\n",
    "prediction = loaded_model.predict(custom_test_image)\n",
    "prediction_prob = prediction[0]\n",
    "\n",
    "emotion_label = np.argmax(prediction[0])\n",
    "\n",
    "# Map the predicted label to emotion class\n",
    "emotion_classes = {0: 'happy', 1: 'sad', 2: 'angry'}\n",
    "predicted_emotion = emotion_classes[emotion_label]\n",
    "\n",
    "# Print the custom test image and its predicted label\n",
    "print(f\"Predicted Emotion: {predicted_emotion}\")\n",
    "print(f\"Confidence [happy, sad, angry]: {prediction_prob}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Display the custom test image using matplotlib\n",
    "plt.imshow(custom_test_image[0, :, :, 0])\n",
    "plt.title(f\"Predicted Emotion: {predicted_emotion}\")\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()\n",
    "\n",
    "from PIL import Image\n",
    "# Display the original custom test image using PIL\n",
    "img_pil = Image.open(custom_test_image_path)\n",
    "plt.imshow(np.array(img_pil))\n",
    "plt.title(f\"Predicted Emotion: {predicted_emotion}\")\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "* Feature Extraction: This approach uses very basic feature extraction (flattening the image), which might not capture the necessary details for accurate emotion classification.\n",
    "* Model Complexity: Logistic regression is quite basic for image classification tasks.\n",
    "* Data Quality: The quality and size of your dataset will significantly impact the performance of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
