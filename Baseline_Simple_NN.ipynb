{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "Simple neural netowrk with self-defined logistic regression, linear regression, regularized logistic regression, regularized linear regression, sigmoid function, ReLU function, and softmax function.\n",
    "\n",
    "### Dataset Analysis:\n",
    "the dataset includes cat, dog and other species. additionally, an image may contains two species. \n",
    "* we should first do face detection from line segments, to segments grouping in order to form a small region or part of the face, to forming an entire face by grouping based on the regions. in the following layer, the model should be able to identify if the face is one of cat, dog, or wild animals. in this part, there will be at least 3 layers given there will be at least one for line segments, at least one for segment grouping into region, amd at least one for face forming based on regions.\n",
    "* in this case, in certain step we shoudl identify if the picture contains cat, dog, and e.g. rabbit or no. so the here it should be binary classification for each unit(neuron). e.g. only two classes, cat and dog. output y = [ bool_has_cat, bool_has_dog ]. Hence this layer should be for instance using sigmoid as activation function.\n",
    "* Then, in the following layer, emotion should be identified to learn that y = [cat happy?, dog happy?, cat sad?, dog sad?, cat angry?, dog angry?, cat relax?, dog relax?] which given each emotional class we should be answering yes or no. if the image is without cat, then all emotional classes relating to cat should be zero. if the image contains cat and dog, emotional classes relating cat and dog should at least have two ones such that cat has certain emotoin relating to a emotion category, and dog should also have certain emotion relating to certain category. it is possible that the specie is happy and sad at the same time such that there are more than two ones. It is due to this that for each layer, it should for instance use sigmoid activatoin function.\n",
    "* given that the chance of having that emotion category is approximated, now this follwoing layer should further learn the level of emotion for each emotional class. for instance, if the image contains cat and dog and that for cat the happy and relaxed are both detected, the two neurons for cat happy and cat relaxed should one compute the level of happy and the other one compute the level of relaxed. the same logic applies to dog. in this case, the function should be ReLU. e.g. y = [level of cat is happy, level of dog is happy, level of cat is sad, level of dog is sad, level of cat is angry, level of dog is angry, level of cat is relaxed, level of dog is relaxed]. if the image contains only cat, then all emtional classes level relating to dog should be zero.\n",
    "* now this layer should identify based on the level of each emotional class. for instance, let's say, the input to this layer is = [level of cat is happy, level of dog is happy, level of cat is sad, level of dog is sad, level of cat is angry, level of dog is angry, level of cat is relaxed, level of dog is relaxed] and that the image contains both cat and dog. cat is detected as happy and relaxed, dog is detected as happy and sad, the input to this layer is then = [0.5, 0.4, 0.0, 0.5, 0.0, 0.0, 0.7, 0.0], then the output of this layer should contains only two ones being [0, 0, 0, 1, 0, 0, 1, 0] such that the cat is relaxed and dog is sad. this layer hence should use either softmax, or, for each unit, for instance, a sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Class\n",
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_derivative = None\n",
    "        self.final_layer = None\n",
    "        self.loss_history = []\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def set_loss(self, loss, loss_derivative):\n",
    "        self.loss = loss\n",
    "        self.loss_derivative = loss_derivative\n",
    "\n",
    "    def forward(self, X):\n",
    "        output = X\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output)\n",
    "        return output\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        for layer in reversed(self.layers):\n",
    "            output_error = layer.backward(output_error, learning_rate)\n",
    "\n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            self.loss_history.append(self.loss(y, output))\n",
    "            output_error = self.loss_derivative(y, output)\n",
    "            self.backward(output_error, learning_rate)\n",
    "            \n",
    "    def add_final_layer(self, layer):\n",
    "        self.final_layer = layer\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------  Activation Functions and Derivatives ----------------------\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Args:\n",
    "        z (ndarray): A scalar, numpy array of any size.\n",
    "\n",
    "    Returns:\n",
    "        g (ndarray): sigmoid(z), with the same shape as z\n",
    "         \n",
    "    \"\"\"\n",
    "\n",
    "    g = 1/(1+np.exp(-z))\n",
    "   \n",
    "    return g\n",
    "  \n",
    "def sigmoid_derivative(output):\n",
    "    return output * (1 - output)\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def regularized_logistic_regression(X, y, weights, lambda_reg):\n",
    "    m = len(y)\n",
    "    predictions = sigmoid(np.dot(X, weights))\n",
    "    error = (-y * np.log(predictions)) - ((1 - y) * np.log(1 - predictions))\n",
    "    cost = np.sum(error) / m\n",
    "    regularization = (lambda_reg / (2 * m)) * np.sum(weights**2)\n",
    "    return cost + regularization\n",
    "\n",
    "# ---------------------------------  Normalization Functions ---------------------------------\n",
    "def normalize_data(X):\n",
    "    return (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "# ---------------------------------  Loss Functions ---------------------------------\n",
    "def compute_cost(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    cost = -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)) / m\n",
    "    return cost\n",
    "\n",
    "def compute_cost_derivative(y_true, y_pred):\n",
    "    return y_pred - y_true\n",
    "  \n",
    "# compute_cost for singular layer logistic regression\n",
    "def compute_cost_logistic(X, y, w, b, *argv):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      y : (ndarray Shape (m,))  target value \n",
    "      w : (ndarray Shape (n,))  values of parameters of the model      \n",
    "      b : (scalar)              value of bias parameter of the model\n",
    "      *argv : unused, for compatibility with regularized version below\n",
    "    Returns:\n",
    "      total_cost : (scalar) cost \n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # first calculate Z = w[0]*X[i][0]+...+w[n-1]*X[i][n-1]+b\n",
    "    Z = np.dot(X,w) + b\n",
    "    F_wb = sigmoid(Z)\n",
    "    \n",
    "    total_cost = (np.dot(-y,np.log(F_wb)) - np.dot(1-y,np.log(1-F_wb)))/m\n",
    "    \n",
    "    ### END CODE HERE ### \n",
    "\n",
    "    return total_cost\n",
    "\n",
    "\n",
    "# compute_gradient for singular layer logistic regression\n",
    "def compute_gradient_logistic(X, y, w, b, *argv): \n",
    "    \"\"\"\n",
    "    Computes the gradient for logistic regression \n",
    " \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      y : (ndarray Shape (m,))  target value \n",
    "      w : (ndarray Shape (n,))  values of parameters of the model      \n",
    "      b : (scalar)              value of bias parameter of the model\n",
    "      *argv : unused, for compatibility with regularized version below\n",
    "    Returns\n",
    "      dj_dw : (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db : (scalar)             The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros(w.shape)\n",
    "    dj_db = 0.\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    # calcualte z for the sigmoid function\n",
    "    Z = np.dot(X,w) + b\n",
    "    F_wb = sigmoid(Z)\n",
    "    dj_dw = np.dot((F_wb - y),X)/m \n",
    "    dj_db = np.sum(F_wb - y)/m\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "        \n",
    "    return dj_db, dj_dw\n",
    "\n",
    "# compute_cost for regularized logistic regression\n",
    "def compute_cost_reg(X, y, w, b, lambda_ = 1):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      y : (ndarray Shape (m,))  target value \n",
    "      w : (ndarray Shape (n,))  values of parameters of the model      \n",
    "      b : (scalar)              value of bias parameter of the model\n",
    "      lambda_ : (scalar, float) Controls amount of regularization\n",
    "    Returns:\n",
    "      total_cost : (scalar)     cost \n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Calls the compute_cost function that you implemented above\n",
    "    cost_without_reg = compute_cost_logistic(X, y, w, b) \n",
    "    \n",
    "    # You need to calculate this value\n",
    "    reg_cost = 0.\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    reg_cost = (np.dot(w,w))* lambda_ / (2*m)\n",
    "        \n",
    "    \n",
    "    ### END CODE HERE ### \n",
    "    \n",
    "    # Add the regularization cost to get the total cost\n",
    "    total_cost = cost_without_reg + reg_cost\n",
    "\n",
    "    return total_cost\n",
    "\n",
    "# compute the gradient for regularized logistic regression\n",
    "def compute_gradient_reg(X, y, w, b, lambda_ = 1): \n",
    "    \"\"\"\n",
    "    Computes the gradient for logistic regression with regularization\n",
    " \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      y : (ndarray Shape (m,))  target value \n",
    "      w : (ndarray Shape (n,))  values of parameters of the model      \n",
    "      b : (scalar)              value of bias parameter of the model\n",
    "      lambda_ : (scalar,float)  regularization constant\n",
    "    Returns\n",
    "      dj_db : (scalar)             The gradient of the cost w.r.t. the parameter b. \n",
    "      dj_dw : (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w. \n",
    "\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    \n",
    "    dj_db, dj_dw = compute_gradient_reg(X, y, w, b)\n",
    "\n",
    "    ### START CODE HERE ###     \n",
    "    dj_dw += np.dot((lambda_/m),w)\n",
    "    ### END CODE HERE ###         \n",
    "        \n",
    "    return dj_db, dj_dw\n",
    "  \n",
    "# ---------------------------------  Gradient Descent Functions ---------------------------------\n",
    "# gradient descent to learn w and b and update them with each iteration\n",
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X :    (ndarray Shape (m, n) data, m examples by n features\n",
    "      y :    (ndarray Shape (m,))  target value \n",
    "      w_in : (ndarray Shape (n,))  Initial values of parameters of the model\n",
    "      b_in : (scalar)              Initial value of parameter of the model\n",
    "      cost_function :              function to compute cost\n",
    "      gradient_function :          function to compute gradient\n",
    "      alpha : (float)              Learning rate\n",
    "      num_iters : (int)            number of iterations to run gradient descent\n",
    "      lambda_ : (scalar, float)    regularization constant\n",
    "      \n",
    "    Returns:\n",
    "      w : (ndarray Shape (n,)) Updated values of parameters of the model after\n",
    "          running gradient descent\n",
    "      b : (scalar)                Updated value of parameter of the model after\n",
    "          running gradient descent\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of training examples\n",
    "    m = len(X)\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w_history = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w_in = w_in - alpha * dj_dw               \n",
    "        b_in = b_in - alpha * dj_db              \n",
    "       \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            cost =  cost_function(X, y, w_in, b_in, lambda_)\n",
    "            J_history.append(cost)\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):\n",
    "            w_history.append(w_in)\n",
    "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
    "        \n",
    "    return w_in, b_in, J_history, w_history #return w and J,w history for graphing\n",
    "\n",
    "\n",
    "# ---------------------------------  Predict Functions ---------------------------------\n",
    "# predict for singular layer logistic regression\n",
    "def predict_logistic(X, w, b): \n",
    "    \"\"\"\n",
    "    Predict whether the label is 0 or 1 using learned logistic\n",
    "    regression parameters w\n",
    "    \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      w : (ndarray Shape (n,))  values of parameters of the model      \n",
    "      b : (scalar)              value of bias parameter of the model\n",
    "\n",
    "    Returns:\n",
    "      p : (ndarray (m,)) The predictions for X using a threshold at 0.5\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m, n = X.shape   \n",
    "    p = np.zeros(m)\n",
    "   \n",
    "    ### START CODE HERE ### \n",
    "    # Loop over each example\n",
    "    Z = np.dot(X,w) + b\n",
    "    # Calculate the prediction for this example\n",
    "    F_wb = sigmoid(Z)\n",
    "\n",
    "    # Apply the threshold\n",
    "    p = F_wb >= 0.5\n",
    "    p.astype(int)\n",
    "        \n",
    "    ### END CODE HERE ### \n",
    "    return p\n",
    "\n",
    "\n",
    "# ---------------------------------  Softmax Function ---------------------------------\n",
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Compute the softmax of z\n",
    "\n",
    "    Args:\n",
    "        z (ndarray): A scalar, numpy array of any size.\n",
    "\n",
    "    Returns:\n",
    "        g (ndarray): softmax(z), with the same shape as z\n",
    "         \n",
    "    \"\"\"\n",
    "    ez = np.exp(z)      #element-wise exponenial\n",
    "    a = ez/np.sum(ez)\n",
    "    return(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------  Dense layer class ----------------------\n",
    "class Dense:\n",
    "    def __init__(self, input_size, output_size, activation, activation_derivative):\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
    "        self.bias = np.zeros((1, output_size))\n",
    "        self.activation = activation\n",
    "        self.activation_derivative = activation_derivative\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.input_error = None\n",
    "        self.weights_error = None\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(input_data, self.weights) + self.bias\n",
    "        if self.activation is not None:\n",
    "            self.output = self.activation(self.output)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        self.input_error = np.dot(output_error, self.weights.T)\n",
    "        self.weights_error = np.dot(self.input.T, output_error)\n",
    "\n",
    "        # Update parameters\n",
    "        self.weights -= learning_rate * self.weights_error\n",
    "        self.bias -= learning_rate * np.sum(output_error, axis=0, keepdims=True)\n",
    "        return self.input_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data pre-processing\n",
    "* Since in the baseline model insteresed implementation should use conventional model, we'll stick with simple neural networks from draft for educational purposes, although it's important to note that this approach will possibily yield a very poor result for such a complex image processing task.\n",
    "* First, you need to preprocess your images. This involves loading the images, resizing them to a uniform size, converting them to grayscale, and flattening them into vectors.\n",
    "\n",
    "#### Benefits of Using Grayscale Images\n",
    "Reduced Complexity: Grayscale images are less complex than color images, making them easier to process with simpler algorithms.\n",
    "Reduced Computational Load: Grayscale images require less computational power and memory, as they have only one channel compared to three in color images.\n",
    "Focus on Texture and Shape: Converting to grayscale can help the model focus on the texture and shape information, which might be more relevant for certain tasks like emotion detection in animals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder, label):\n",
    "    images = []\n",
    "    labels = []\n",
    "    file_paths = []  # List to store file paths\n",
    "    for filename in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, filename)  # Get the full file path\n",
    "        img = cv2.imread(file_path)\n",
    "        if img is not None:\n",
    "            gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "            img_resized = cv2.resize(gray_img, (32, 32))  # Resize images\n",
    "            images.append(img_resized.flatten())\n",
    "            labels.append(label)\n",
    "            file_paths.append(file_path)  # Store the file path for prediction visualization\n",
    "    return images, labels, file_paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Labeling the Data\n",
    "If one has dataset for training and is without labels, one will need to assign labels to the data. Since we have separate folders for each emotion and labels for each data, we can skipp this part.\n",
    "#### Define the path to the sub dataset folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the dataset folders\n",
    "happy_folder = \"/kaggle/input/pets-facial-expression-dataset/happy\"\n",
    "sad_folder = \"/kaggle/input/pets-facial-expression-dataset/Sad\"\n",
    "angry_folder = \"/kaggle/input/pets-facial-expression-dataset/Angry\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data and Combine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "happy_images, happy_labels, happy_file_paths = load_images_from_folder(happy_folder, 0)  # Label 0 for happy\n",
    "sad_images, sad_labels, sad_file_paths = load_images_from_folder(sad_folder, 1)  # Label 1 for sad\n",
    "angry_images, angry_labels, angry_file_paths = load_images_from_folder(angry_folder, 2)  # Label 2 for angry\n",
    "\n",
    "# Combine data\n",
    "X = np.array(happy_images + sad_images + angry_images)\n",
    "y = np.array(happy_labels + sad_labels + angry_labels)\n",
    "\n",
    "# Show the shape of the data\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Splitting the Data\n",
    "Split dataset into training and testing sets. This is essential for evaluating the performance of the model.\n",
    "A common split is 80% for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Normalize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "X_train_normalized = X_train / 255.0\n",
    "X_test_normalized = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Functions\n",
    "Use sigmoid function, ReLU, and softmax to build the simple neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Refresher on logistic regression and decision boundary\n",
    "\n",
    "* Recall that for logistic regression, the model is represented as \n",
    "\n",
    "  $$f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b) \\tag{1}$$\n",
    "\n",
    "  where $g(z)$ is known as the sigmoid function and it maps all input values to values between 0 and 1:\n",
    "\n",
    "  $$g(z) = \\frac{1}{1+e^{-z}}\\tag{2}$$\n",
    "  and $\\mathbf{w} \\cdot \\mathbf{x}$ is the vector dot product:\n",
    "  \n",
    "  $$\\mathbf{w} \\cdot \\mathbf{x} = w_0 x_0 + w_1 x_1$$\n",
    "  \n",
    "  \n",
    " * We interpret the output of the model ($f_{\\mathbf{w},b}(x)$) as the probability that $y=1$ given $\\mathbf{x}$ and parameterized by $\\mathbf{w}$ and $b$.\n",
    "* Therefore, to get a final prediction ($y=0$ or $y=1$) from the logistic regression model, we can use the following heuristic -\n",
    "\n",
    "  if $f_{\\mathbf{w},b}(x) >= 0.5$, predict $y=1$\n",
    "  \n",
    "  if $f_{\\mathbf{w},b}(x) < 0.5$, predict $y=0$\n",
    "  \n",
    "  \n",
    "* Let's plot the sigmoid function to see where $g(z) >= 0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Cost function\n",
    "\n",
    "In a previous lab, you developed the *logistic loss* function. Recall, loss is defined to apply to one example. Here you combine the losses to form the **cost**, which includes all the examples.\n",
    "\n",
    "\n",
    "Recall that for logistic regression, the cost function is of the form \n",
    "\n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$\n",
    "\n",
    "where\n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is:\n",
    "\n",
    "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$\n",
    "    \n",
    "*  where m is the number of training examples in the data set and:\n",
    "$$\n",
    "\\begin{align}\n",
    "  f_{\\mathbf{w},b}(\\mathbf{x^{(i)}}) &= g(z^{(i)})\\tag{3} \\\\\n",
    "  z^{(i)} &= \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+ b\\tag{4} \\\\\n",
    "  g(z^{(i)}) &= \\frac{1}{1+e^{-z^{(i)}}}\\tag{5} \n",
    "\\end{align}\n",
    "$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Gradient for logistic regression\n",
    "\n",
    "In this section, you will implement the gradient for logistic regression.\n",
    "\n",
    "Recall that the gradient descent algorithm is:\n",
    "\n",
    "$$\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & b := b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\newline       \\; & w_j := w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1}\\newline & \\rbrace\\end{align*}$$\n",
    "\n",
    "where, parameters $b$, $w_j$ are all updated simultaniously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `compute_gradient` function to compute $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$, $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ from equations (2) and (3) below.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)}) \\tag{2}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})x_{j}^{(i)} \\tag{3}\n",
    "$$\n",
    "* m is the number of training examples in the dataset\n",
    "\n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(x^{(i)})$ is the model's prediction, while $y^{(i)}$ is the actual label\n",
    "\n",
    "\n",
    "- **Note**: While this gradient looks identical to the linear regression gradient, the formula is actually different because linear and logistic regression have different definitions of $f_{\\mathbf{w},b}(x)$.\n",
    "\n",
    "As before, you can use the sigmoid function that you implemented above and if you get stuck, you can check out the hints presented after the cell below to help you with the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Learning parameters using gradient descent \n",
    "\n",
    "Similar to the previous assignment, you will now find the optimal parameters of a logistic regression model by using gradient descent. \n",
    "- You don't need to implement anything for this part. Simply run the cells below. \n",
    "\n",
    "- A good way to verify that gradient descent is working correctly is to look\n",
    "at the value of $J(\\mathbf{w},b)$ and check that it is decreasing with each step. \n",
    "\n",
    "- Assuming you have implemented the gradient and computed the cost correctly, your value of $J(\\mathbf{w},b)$ should never increase, and should converge to a steady value by the end of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the gradient descent algorithm above to learn the parameters for our dataset.\n",
    "\n",
    "**Note**\n",
    "The code block below takes a couple of minutes to run, especially with a non-vectorized version. You can reduce the `iterations` to test your implementation and iterate faster. If you have time later, try running 100,000 iterations for better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Predict\n",
    "\n",
    "Please complete the `predict` function to produce `1` or `0` predictions given a dataset and a learned parameter vector $w$ and $b$.\n",
    "- First you need to compute the prediction from the model $f(x^{(i)}) = g(w \\cdot x^{(i)} + b)$ for every example \n",
    "    - You've implemented this before in the parts above\n",
    "- We interpret the output of the model ($f(x^{(i)})$) as the probability that $y^{(i)}=1$ given $x^{(i)}$ and parameterized by $w$.\n",
    "- Therefore, to get a final prediction ($y^{(i)}=0$ or $y^{(i)}=1$) from the logistic regression model, you can use the following heuristic -\n",
    "\n",
    "  if $f(x^{(i)}) >= 0.6$, predict $y^{(i)}=1$\n",
    "  \n",
    "  if $f(x^{(i)}) < 0.6$, predict $y^{(i)}=0$\n",
    "    \n",
    "If you get stuck, you can check out the hints presented after the cell below to help you with the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Regularized Logistic Regression\n",
    "\n",
    "In this part of the exercise, you will implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly. \n",
    "\n",
    "### 5.6.1 Problem Statement\n",
    "\n",
    "Suppose you are the product manager of the factory and you have the test results for some microchips on two different tests. \n",
    "- From these two tests, you would like to determine whether the microchips should be accepted or rejected. \n",
    "- To help you make the decision, you have a dataset of test results on past microchips, from which you can build a logistic regression model.\n",
    "\n",
    "\n",
    "### 5.6.2 Loading and visualizing the data\n",
    "\n",
    "Similar to previous parts of this exercise, let's start by loading the dataset for this task and visualizing it. \n",
    "\n",
    "- The `load_dataset()` function shown below loads the data into variables `X_train` and `y_train`\n",
    "  - `X_train` contains the test results for the microchips from two tests\n",
    "  - `y_train` contains the results of the QA  \n",
    "      - `y_train = 1` if the microchip was accepted \n",
    "      - `y_train = 0` if the microchip was rejected \n",
    "  - Both `X_train` and `y_train` are numpy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the feature mapping allows us to build a more expressive classifier, it is also more susceptible to overfitting. In the next parts of the exercise, you will implement regularized logistic regression to fit the data and also see for yourself how regularization can help combat the overfitting problem.\n",
    "\n",
    "### 5.7 Cost function for regularized logistic regression\n",
    "\n",
    "In this part, you will implement the cost function for regularized logistic regression.\n",
    "\n",
    "Recall that for regularized logistic regression, the cost function is of the form\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{m}  \\sum_{i=0}^{m-1} \\left[ -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$$\n",
    "\n",
    "Compare this to the cost function without regularization (which you implemented above), which is of the form \n",
    "\n",
    "$$ J(\\mathbf{w}.b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\right]$$\n",
    "\n",
    "The difference is the regularization term, which is $$\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$$ \n",
    "Note that the $b$ parameter is not regularized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5.8 Compute Regularized Logistic Regression\n",
    "\n",
    "Please complete the `compute_cost_reg` function below to calculate the following term for each element in $w$ \n",
    "$$\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$$\n",
    "\n",
    "The starter code then adds this to the cost without regularization (which you computed above in `compute_cost`) to calculate the cost with regulatization.\n",
    "\n",
    "If you get stuck, you can check out the hints presented after the cell below to help you with the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.9 Gradient for regularized logistic regression\n",
    "\n",
    "In this section, you will implement the gradient for regularized logistic regression.\n",
    "\n",
    "\n",
    "The gradient of the regularized cost function has two components. The first, $$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$$ is a scalar, the other is a vector with the same shape as the parameters $\\mathbf{w}$, where the $j^\\mathrm{th}$ element is defined as follows:\n",
    "\n",
    "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b} = \\frac{1}{m}  \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})$$\n",
    "\n",
    "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} = \\left( \\frac{1}{m}  \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)} \\right) + \\frac{\\lambda}{m} w_j  \\quad\\, \\text{for $j=0...(n-1)$}$$\n",
    "\n",
    "Compare this to the gradient of the cost function without regularization (which you implemented above), which is of the form \n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)}) \\tag{2}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})x_{j}^{(i)} \\tag{3}\n",
    "$$\n",
    "\n",
    "\n",
    "As you can see,$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ is the same, the difference is the following term in $$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$$, which is $$\\frac{\\lambda}{m} w_j  \\quad\\, \\text{for $j=0...(n-1)$}$$ \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Define the Network Architecture\n",
    "1. Face Detection and Segmentation\n",
    "* **Objective**: Detect and segment animal faces from images.\n",
    "* **Approach**: This typically requires convolutional neural networks (CNNs) that can identify patterns (like edges, textures) and group them into larger structures (like faces).\n",
    "* **Layers**: Start with convolutional layers for feature extraction, followed by pooling layers to reduce dimensionality, and fully connected layers for classification.\n",
    "Activation Functions: ReLU is commonly used in CNNs for its efficiency.\n",
    "2. Species Identification\n",
    "* **Objective**: Identify whether the image contains a cat, dog, or other species.\n",
    "* **Approach**: This is a multi-label classification problem (since an image can have more than one label).\n",
    "* **Layers**: Fully connected layers following the feature extraction layers.\n",
    "* **Activation Function**:** Sigmoid activation function for each neuron (since it's a binary classification for each species).\n",
    "3. Emotion Detection\n",
    "* **Objective**: Identify emotions of each species present in the image.\n",
    "* **Approach**: This is again a multi-label classification problem.\n",
    "Layers: Additional fully connected layers.\n",
    "* **Activation Function**: Sigmoid, as each emotion (happy, sad, etc.) is treated as a separate binary classification problem.\n",
    "4. Emotion Intensity Level\n",
    "* **Objective**: Determine the intensity level of each detected emotion.\n",
    "* **Approach**: This can be seen as a regression problem for each emotion.\n",
    "* **Layers**: Fully connected layers.\n",
    "* **Activation Function**: ReLU or a similar function, as the output is a non-negative intensity level.\n",
    "5. Final Layer for Emotion Classification\n",
    "* **Objective**: Classify the predominant emotion for each species.\n",
    "* **Approach**: This is a classification problem, but with a twist. You're interested in the predominant emotion, which is a bit different from standard classification.\n",
    "* **Layers**: Fully connected layer.\n",
    "* **Activation Function**: Softmax if you're classifying one predominant emotion per species, or sigmoid for binary classification of each emotion.\n",
    "\n",
    "### Additional Considerations:\n",
    "* **Data Preprocessing**: Ensure images are properly preprocessed (normalized, resized, etc.).\n",
    "* **Model Complexity**: This is a complex model. Start with a simpler version and iteratively add complexity.\n",
    "* **Training Data**: You'll need a large and well-labeled dataset for this task, especially for the emotion detection and intensity levels.\n",
    "* **Evaluation Metrics**: Choose appropriate metrics for each stage (accuracy, F1 score, mean squared error for intensity levels, etc.).\n",
    "* **Computational Resources**: This model might require significant computational resources, especially for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Configure the NN\n",
    "Since we have four output classes (happy, sad, angry, relaxed), the last output layer should have 4 neurons with a softmax activation function for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming each image is flattened to a vector of size 1024 (32x32)\n",
    "input_size = 1024\n",
    "network = SimpleNeuralNetwork()\n",
    "\n",
    "# Add layers\n",
    "# Network Structure Proposal:\n",
    "# Input Layer: Size depends on the preprocessed image dimensions.\n",
    "# Hidden Layers: A series of dense layers with ReLU activation for feature extraction.\n",
    "# Output Layer for Species Classification: ReLU/Sigmoid activation for multi-class classification.\n",
    "# Output Layer for Emotion Detection: ReLU/Sigmoid/Sigmoid activation for multi-label classification.\n",
    "# Output Layer for Emotion Intensity Level: ReLU activation for regression of intensity levels.\n",
    "# Assuming input size is determined by preprocessed image size\n",
    "\n",
    "network.add_layer(Dense(input_size, 512, relu, relu_derivative))\n",
    "network.add_layer(Dense(512, 256, relu, relu_derivative))\n",
    "network.add_layer(Dense(256, 128, relu, relu_derivative))\n",
    "network.add_layer(Dense(128, 64, relu, relu_derivative))\n",
    "network.add_layer(Dense(64, 48, relu, relu_derivative))\n",
    "\n",
    "'''Task-Specific Branches: \n",
    "    After the common layers, \n",
    "    create separate branches for each task. \n",
    "    Each branch will have its own layers that are specifically designed for that task.\n",
    "'''\n",
    "# # Branch for Species Classification\n",
    "# species_network = SimpleNeuralNetwork() \n",
    "# species_network.add_layer(Dense(48, 3, softmax, None))  # Softmax/ReLU/sigmoid for species classification (3 species: cat, dog, others)\n",
    "\n",
    "# # Branch for Emotion Detection\n",
    "# emotion_detection_network = SimpleNeuralNetwork()\n",
    "# emotion_detection_network.add_layer(Dense(48, 4, sigmoid, sigmoid_derivative))  # Sigmoid for emotion detection\n",
    "\n",
    "# # Branch for Emotion Intensity Level\n",
    "# emotion_intensity_network = SimpleNeuralNetwork()\n",
    "# emotion_intensity_network.add_layer(Dense(48, 4, relu, relu_derivative))  # ReLU for emotion intensity level\n",
    "\n",
    "# # Set loss function - cross-entropy for classification tasks\n",
    "# network.set_loss(compute_cost, compute_cost_derivative)\n",
    "\n",
    "# '''Connecting the Branches:\n",
    "#     ensure that the output of the common feature extraction base is fed into each of the task-specific branches. \n",
    "#     This can be done during the forward pass of the network.\n",
    "# '''\n",
    "# # Assuming 'X_train_normalization' is your input\n",
    "# common_features = network.forward(X_train_normalized)\n",
    "\n",
    "# # Using common features in each branch\n",
    "# species_output = species_network.forward(common_features)\n",
    "# emotion_detection_output = emotion_detection_network.forward(common_features)\n",
    "# emotion_intensity_output = emotion_intensity_network.forward(common_features)\n",
    "\n",
    "# # Combine outputs and pass through final layer\n",
    "# combined_output = np.concatenate((species_output, emotion_detection_output, emotion_intensity_output))\n",
    "# final_layer = Dense(combined_output.size, 8, softmax, None)\n",
    "# # add the final layer to the network\n",
    "# network.add_final_layer(final_layer)\n",
    "# # final_output = final_layer.forward(combined_output)\n",
    "# final_output = network.final_layer.forward(combined_output)\n",
    "\n",
    "# # combine the loss functions\n",
    "# def combined_loss(y_true, y_pred):\n",
    "#     species_loss = compute_cost(y_true[:, 0:3], y_pred[:, 0:3])\n",
    "#     emotion_detection_loss = compute_cost(y_true[:, 3:7], y_pred[:, 3:7])\n",
    "#     emotion_intensity_loss = compute_cost(y_true[:, 7:8], y_pred[:, 7:8])\n",
    "#     return species_loss + emotion_detection_loss + emotion_intensity_loss\n",
    "\n",
    "# # combine the loss derivatives\n",
    "# def combined_loss_derivative(y_true, y_pred):\n",
    "#     species_loss_derivative = compute_cost_derivative(y_true[:, 0:3], y_pred[:, 0:3])\n",
    "#     emotion_detection_loss_derivative = compute_cost_derivative(y_true[:, 3:7], y_pred[:, 3:7])\n",
    "#     emotion_intensity_loss_derivative = compute_cost_derivative(y_true[:, 7:8], y_pred[:, 7:8])\n",
    "#     return np.concatenate((species_loss_derivative, emotion_detection_loss_derivative, emotion_intensity_loss_derivative), axis=1)\n",
    "\n",
    "# # set the loss function and loss derivative\n",
    "# network.set_loss(combined_loss, combined_loss_derivative)\n",
    "\n",
    "\n",
    "'''Without branching, the network will have a single output layer.\n",
    "    The output layer will have 12 nodes,\n",
    "    with the each set of four nodes representing the classified emotion of the corresponding species. \n",
    "    output = [cat_happy, cat_sad, cat_angry, cat_neutral, dog_happy, dog_sad, dog_angry, dog_neutral, others_happy, others_sad, others_angry, others_neutral]\n",
    "'''\n",
    "network.add_layer(Dense(48, 3, sigmoid, sigmoid_derivative)) # ReLU/Sigmoid for species classification (3 species: cat, dog, others)\n",
    "network.add_layer(Dense(3, 4, sigmoid, sigmoid_derivative)) # Sigmoid for emotion detection\n",
    "network.add_layer(Dense(4, 4, relu, relu_derivative)) # ReLU for emotion intensity level\n",
    "network.add_layer(Dense(4, 12, softmax, None)) # Softmax for to get probabilities for each emotion for each species\n",
    "\n",
    "# Set loss function - cross-entropy for classification tasks\n",
    "network.set_loss(compute_cost, compute_cost_derivative)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Train the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network\n",
    "epochs = 1000\n",
    "learning_rate = 0.01\n",
    "network.train(X_train_normalized, y_train, epochs, learning_rate)\n",
    "\n",
    "# Plot the loss history\n",
    "plt.plot(network.loss_history)\n",
    "plt.title(\"Loss History\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "# calculate training accuracy\n",
    "y_pred = network.forward(X_train_normalized)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "accuracy = np.mean(y_pred == y_train)\n",
    "print(f\"Training Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Predict and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "predictions = network.forward(X_test_normalized)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "'''For evaluation, you'll need to compare the predictions with the true labels. The specific evaluation metrics depend on the nature of your tasks (classification, multi-label classification, regression).'''\n",
    "'''\n",
    "Classification (Species): Use accuracy, precision, recall, F1-score, etc.\n",
    "Multi-label Classification (Emotion Detection): Use accuracy, Hamming loss, etc.\n",
    "Regression (Emotion Intensity Level): Use mean squared error, mean absolute error, etc.\n",
    "'''\n",
    "\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "# Plot the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualizing the prediction result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "def visualize_predictions(file_paths, predictions, true_labels=None, num_images=10):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(num_images):\n",
    "        img = cv2.imread(file_paths[i], cv2.IMREAD_GRAYSCALE)\n",
    "        img_resized = cv2.resize(img, (32, 32))\n",
    "        plt.subplot(2, num_images // 2, i + 1)\n",
    "        plt.imshow(img_resized, cmap='gray')\n",
    "        title = f\"Pred: {predictions[i]}\"\n",
    "        if true_labels is not None:\n",
    "            title += f\"\\nTrue: {true_labels[i]}\"\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'file_paths' is the list of file paths for the test dataset\n",
    "file_paths = happy_file_paths + sad_file_paths + angry_file_paths\n",
    "visualize_predictions(file_paths, y_pred, y_test, num_images=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "* Feature Extraction: This approach uses very basic feature extraction (flattening the image), which might not capture the necessary details for accurate emotion classification.\n",
    "* Model Complexity: Logistic regression is quite basic for image classification tasks.\n",
    "* Data Quality: The quality and size of your dataset will significantly impact the performance of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
