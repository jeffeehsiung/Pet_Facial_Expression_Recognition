{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "Simple neural netowrk with self-defined logistic regression, linear regression, regularized logistic regression, regularized linear regression, sigmoid function, ReLU function, and softmax function.\n",
    "\n",
    "### Dataset Analysis:\n",
    "the dataset includes cat, dog and other species. additionally, an image may contains two species. \n",
    "* we should first do face detection from line segments, to segments grouping in order to form a small region or part of the face, to forming an entire face by grouping based on the regions. in the following layer, the model should be able to identify if the face is one of cat, dog, or wild animals. in this part, there will be at least 3 layers given there will be at least one for line segments, at least one for segment grouping into region, amd at least one for face forming based on regions.\n",
    "* in this case, in certain step we shoudl identify if the picture contains cat, dog, and e.g. rabbit or no. so the here it should be binary classification for each unit(neuron). e.g. only two classes, cat and dog. output y = [ bool_has_cat, bool_has_dog ]. Hence this layer should be for instance using sigmoid as activation function.\n",
    "* Then, in the following layer, emotion should be identified to learn that y = [cat happy?, dog happy?, cat sad?, dog sad?, cat angry?, dog angry?, cat relax?, dog relax?] which given each emotional class we should be answering yes or no. if the image is without cat, then all emotional classes relating to cat should be zero. if the image contains cat and dog, emotional classes relating cat and dog should at least have two ones such that cat has certain emotoin relating to a emotion category, and dog should also have certain emotion relating to certain category. it is possible that the specie is happy and sad at the same time such that there are more than two ones. It is due to this that for each layer, it should for instance use sigmoid activatoin function.\n",
    "* given that the chance of having that emotion category is approximated, now this follwoing layer should further learn the level of emotion for each emotional class. for instance, if the image contains cat and dog and that for cat the happy and relaxed are both detected, the two neurons for cat happy and cat relaxed should one compute the level of happy and the other one compute the level of relaxed. the same logic applies to dog. in this case, the function should be ReLU. e.g. y = [level of cat is happy, level of dog is happy, level of cat is sad, level of dog is sad, level of cat is angry, level of dog is angry, level of cat is relaxed, level of dog is relaxed]. if the image contains only cat, then all emtional classes level relating to dog should be zero.\n",
    "* now this layer should identify based on the level of each emotional class. for instance, let's say, the input to this layer is = [level of cat is happy, level of dog is happy, level of cat is sad, level of dog is sad, level of cat is angry, level of dog is angry, level of cat is relaxed, level of dog is relaxed] and that the image contains both cat and dog. cat is detected as happy and relaxed, dog is detected as happy and sad, the input to this layer is then = [0.5, 0.4, 0.0, 0.5, 0.0, 0.0, 0.7, 0.0], then the output of this layer should contains only two ones being [0, 0, 0, 1, 0, 0, 1, 0] such that the cat is relaxed and dog is sad. this layer hence should use either softmax, or, for each unit, for instance, a sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel\n",
    "tf.autograph.set_verbosity(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data pre-processing\n",
    "* Since in the baseline model insteresed implementation should use conventional model, we'll stick with simple neural networks from draft for educational purposes, although it's important to note that this approach will possibily yield a very poor result for such a complex image processing task.\n",
    "* First, you need to preprocess your images. This involves loading the images, resizing them to a uniform size, converting them to grayscale, and flattening them into vectors.\n",
    "\n",
    "#### Benefits of Using Grayscale Images\n",
    "Reduced Complexity: Grayscale images are less complex than color images, making them easier to process with simpler algorithms.\n",
    "Reduced Computational Load: Grayscale images require less computational power and memory, as they have only one channel compared to three in color images.\n",
    "Focus on Texture and Shape: Converting to grayscale can help the model focus on the texture and shape information, which might be more relevant for certain tasks like emotion detection in animals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess images\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            img = cv2.resize(img, (48, 48))  # Resize to a fixed size for the model\n",
    "            images.append(img)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Labeling the Data\n",
    "If one has dataset for training and is without labels, one will need to assign labels to the data. Since we have separate folders for each emotion and labels for each data, we can skipp this part.\n",
    "#### Define the path to the sub dataset folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the dataset folders\n",
    "happy_folder = \"/kaggle/input/pets-facial-expression-dataset/happy\"\n",
    "sad_folder = \"/kaggle/input/pets-facial-expression-dataset/Sad\"\n",
    "angry_folder = \"/kaggle/input/pets-facial-expression-dataset/Angry\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data and Combine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images and labels for each emotion\n",
    "happy_images = load_images_from_folder(happy_folder)\n",
    "sad_images = load_images_from_folder(sad_folder)\n",
    "angry_images = load_images_from_folder(angry_folder)\n",
    "\n",
    "\n",
    "# Create labels for each emotion category\n",
    "happy_labels = [0] * len(happy_images)\n",
    "sad_labels = [1] * len(sad_images)\n",
    "angry_labels = [2] * len(angry_images)\n",
    "\n",
    "\n",
    "# Concatenate images and labels\n",
    "X = np.array(happy_images + sad_images + angry_images)\n",
    "y = np.array(happy_labels + sad_labels + angry_labels)\n",
    "\n",
    "# Normalize pixel values to range [0, 1]\n",
    "X = X.astype('float32') / 255.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "y = np_utils.to_categorical(y, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Splitting the Data\n",
    "Split dataset into training and testing sets. This is essential for evaluating the performance of the model.\n",
    "A common split is 80% for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define the Network Architecture\n",
    "1. Face Detection and Segmentation (line, segments, face)\n",
    "* **Objective**: Detect and segment animal faces from images.\n",
    "* **Approach**: This typically requires convolutional neural networks (CNNs) that can identify patterns (like edges, textures) and group them into larger structures (like faces).\n",
    "* **Layers**: Start with convolutional layers for feature extraction, followed by pooling layers to reduce dimensionality, and fully connected layers for classification.\n",
    "Activation Functions: ReLU is commonly used in CNNs for its efficiency.\n",
    "2. Species Identification (if we have the labels)\n",
    "* **Objective**: Identify whether the image contains a cat, dog, or other species.\n",
    "* **Approach**: This is a multi-label classification problem (since an image can have more than one label).\n",
    "* **Layers**: Fully connected layers following the feature extraction layers.\n",
    "* **Activation Function**:** Sigmoid activation function for each neuron (since it's a binary classification for each species).\n",
    "3. Final Layer for Emotion Classification\n",
    "* **Objective**: Classify the predominant emotion for each species.\n",
    "* **Approach**: This is a classification problem, but with a twist. You're interested in the predominant emotion, which is a bit different from standard classification.\n",
    "* **Layers**: Fully connected layer.\n",
    "* **Activation Function**: Softmax if you're classifying one predominant emotion per species, or sigmoid for binary classification of each emotion.\n",
    "\n",
    "### Additional Considerations:\n",
    "* **Data Preprocessing**: Ensure images are properly preprocessed (normalized, resized, etc.).\n",
    "* **Model Complexity**: This is a complex model. Start with a simpler version and iteratively add complexity.\n",
    "* **Training Data**: You'll need a large and well-labeled dataset for this task, especially for the emotion detection and intensity levels.\n",
    "* **Evaluation Metrics**: Choose appropriate metrics for each stage (accuracy, F1 score, mean squared error for intensity levels, etc.).\n",
    "* **Computational Resources**: This model might require significant computational resources, especially for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Configure the NN\n",
    "Since we have four output classes (happy, sad, angry, relaxed), the last output layer should have 4 neurons with a softmax activation function for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the simple neural netowrk model\n",
    "\n",
    "model = Sequential(\n",
    "    [               \n",
    "        Flatten(input_shape=(48, 48)),  # The input shape is the size of the images,    #specify input size\n",
    "        ### START CODE HERE ### \n",
    "        Dense(512, activation=\"relu\", name=\"line\"),\n",
    "        Dense(256, activation=\"relu\", name=\"segment\"),\n",
    "        Dense(128, activation=\"relu\", name=\"face\"),\n",
    "        Dense(3, activation=\"relu\", name=\"species\"),\n",
    "        Dense(3, activation=\"softmax\", name=\"emotion\"),\n",
    "        ### END CODE HERE ### \n",
    "    ], name = \"pet_emotion\" \n",
    ")\n",
    "\n",
    "[line, segment, face, species, emotion] = model.layers[1:]\n",
    "#### Examine Weights shapes\n",
    "W1,b1 = line.get_weights()\n",
    "W2,b2 = segment.get_weights()\n",
    "W3,b3 = face.get_weights()\n",
    "W4,b4 = species.get_weights()\n",
    "W5,b5 = emotion.get_weights()\n",
    "\n",
    "print(f\"W1 shape = {W1.shape}, b1 shape = {b1.shape}\")\n",
    "print(f\"W2 shape = {W2.shape}, b2 shape = {b2.shape}\")\n",
    "print(f\"W3 shape = {W3.shape}, b3 shape = {b3.shape}\")\n",
    "print(f\"W4 shape = {W4.shape}, b4 shape = {b4.shape}\")\n",
    "print(f\"W5 shape = {W5.shape}, b5 shape = {b5.shape}\")\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss=CategoricalCrossentropy(from_logits=False),\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    metrics=[CategoricalAccuracy()]\n",
    ")\n",
    "\n",
    "# # Define the model\n",
    "# model = Sequential([\n",
    "#     Flatten(input_shape=(48, 48)),\n",
    "#     Dense(512, activation='relu'),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dense(128, activation='relu'),\n",
    "#     Dense(3, activation='softmax')  # Output layer for 3 classes with softmax activation\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(\n",
    "#     loss=CategoricalCrossentropy(from_logits=False),\n",
    "#     optimizer=Adam(learning_rate=0.001),\n",
    "#     metrics=[CategoricalAccuracy()]\n",
    "# )\n",
    "\n",
    "model.save('facial_expression_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine details of the model by first extracting the layers with `model.layers` and then extracting the weights with `layerx.get_weights()` as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Train the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_split=0.1)  # 48 to 0.1874, 100 to 0.05\n",
    "\n",
    "# For training accuracy and loss\n",
    "training_accuracy = history.history['categorical_accuracy']\n",
    "training_loss = history.history['loss']\n",
    "\n",
    "# For validation accuracy and loss\n",
    "validation_accuracy = history.history['val_categorical_accuracy']\n",
    "validation_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict on test data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, 101)  # number of epochs = 1000\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, training_accuracy, 'b-', label='Training accuracy')\n",
    "plt.plot(epochs, validation_accuracy, 'r-', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, training_loss, 'b-', label='Training loss')\n",
    "plt.plot(epochs, validation_loss, 'r-', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Test Angry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "# Load the saved model\n",
    "loaded_model = load_model(\"facial_expression_model.h5\")\n",
    "\n",
    "\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            img = cv2.resize(img, (48, 48))  # Resize to a fixed size for the model\n",
    "            images.append(img)\n",
    "    return images\n",
    "\n",
    "\n",
    "# Load a custom test image\n",
    "custom_test_image_path = \"/kaggle/input/pets-facial-expression-dataset/Angry/35.jpg\"\n",
    "\n",
    "custom_test_image = cv2.imread(custom_test_image_path)\n",
    "custom_test_image = cv2.cvtColor(custom_test_image, cv2.COLOR_BGR2GRAY)\n",
    "custom_test_image = cv2.resize(custom_test_image, (48, 48))\n",
    "custom_test_image = custom_test_image.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the image to match the model input shape\n",
    "custom_test_image = np.expand_dims(custom_test_image, axis=0)\n",
    "custom_test_image = np.expand_dims(custom_test_image, axis=-1)\n",
    "\n",
    "# Make predictions on the custom test image\n",
    "prediction = loaded_model.predict(custom_test_image)\n",
    "prediction_prob = prediction[0]\n",
    "\n",
    "emotion_label = np.argmax(prediction[0])\n",
    "\n",
    "# Map the predicted label to emotion class\n",
    "emotion_classes = {0: 'happy', 1: 'sad', 2: 'angry'}\n",
    "predicted_emotion = emotion_classes[emotion_label]\n",
    "\n",
    "# Print the custom test image and its predicted label\n",
    "print(f\"Predicted Emotion: {predicted_emotion}\")\n",
    "print(f\"Confidence [happy, sad, angry]: {prediction_prob}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Display the custom test image using matplotlib\n",
    "plt.imshow(custom_test_image[0, :, :, 0])\n",
    "plt.title(f\"Predicted Emotion: {predicted_emotion}\")\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()\n",
    "\n",
    "from PIL import Image\n",
    "# Display the original custom test image using PIL\n",
    "img_pil = Image.open(custom_test_image_path)\n",
    "plt.imshow(np.array(img_pil))\n",
    "plt.title(f\"Predicted Emotion: {predicted_emotion}\")\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Test Happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "# Load the saved model\n",
    "loaded_model = load_model(\"facial_expression_model.h5\")\n",
    "\n",
    "\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            img = cv2.resize(img, (48, 48))  # Resize to a fixed size for the model\n",
    "            images.append(img)\n",
    "    return images\n",
    "\n",
    "\n",
    "# Load a custom test image\n",
    "custom_test_image_path = \"/kaggle/input/pets-facial-expression-dataset/happy/003.jpg\"\n",
    "\n",
    "custom_test_image = cv2.imread(custom_test_image_path)\n",
    "custom_test_image = cv2.cvtColor(custom_test_image, cv2.COLOR_BGR2GRAY)\n",
    "custom_test_image = cv2.resize(custom_test_image, (48, 48))\n",
    "custom_test_image = custom_test_image.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the image to match the model input shape\n",
    "custom_test_image = np.expand_dims(custom_test_image, axis=0)\n",
    "custom_test_image = np.expand_dims(custom_test_image, axis=-1)\n",
    "\n",
    "# Make predictions on the custom test image\n",
    "prediction = loaded_model.predict(custom_test_image)\n",
    "prediction_prob = prediction[0]\n",
    "\n",
    "emotion_label = np.argmax(prediction[0])\n",
    "\n",
    "# Map the predicted label to emotion class\n",
    "emotion_classes = {0: 'happy', 1: 'sad', 2: 'angry'}\n",
    "predicted_emotion = emotion_classes[emotion_label]\n",
    "\n",
    "# Print the custom test image and its predicted label\n",
    "print(f\"Predicted Emotion: {predicted_emotion}\")\n",
    "print(f\"Confidence [happy, sad, angry]: {prediction_prob}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display the custom test image using matplotlib\n",
    "plt.imshow(custom_test_image[0, :, :, 0])\n",
    "plt.title(f\"Predicted Emotion: {predicted_emotion}\")\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()\n",
    "\n",
    "from PIL import Image\n",
    "# Display the original custom test image using PIL\n",
    "img_pil = Image.open(custom_test_image_path)\n",
    "plt.imshow(np.array(img_pil))\n",
    "plt.title(f\"Predicted Emotion: {predicted_emotion}\")\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Test Sad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "# Load the saved model\n",
    "loaded_model = load_model(\"facial_expression_model.h5\")\n",
    "\n",
    "\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            img = cv2.resize(img, (48, 48))  # Resize to a fixed size for the model\n",
    "            images.append(img)\n",
    "    return images\n",
    "\n",
    "\n",
    "# Load a custom test image\n",
    "custom_test_image_path = \"/kaggle/input/pets-facial-expression-dataset/Sad/001.jpg\"\n",
    "\n",
    "custom_test_image = cv2.imread(custom_test_image_path)\n",
    "custom_test_image = cv2.cvtColor(custom_test_image, cv2.COLOR_BGR2GRAY)\n",
    "custom_test_image = cv2.resize(custom_test_image, (48, 48))\n",
    "custom_test_image = custom_test_image.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the image to match the model input shape\n",
    "custom_test_image = np.expand_dims(custom_test_image, axis=0)\n",
    "custom_test_image = np.expand_dims(custom_test_image, axis=-1)\n",
    "\n",
    "# Make predictions on the custom test image\n",
    "prediction = loaded_model.predict(custom_test_image)\n",
    "prediction_prob = prediction[0]\n",
    "\n",
    "emotion_label = np.argmax(prediction[0])\n",
    "\n",
    "# Map the predicted label to emotion class\n",
    "emotion_classes = {0: 'happy', 1: 'sad', 2: 'angry'}\n",
    "predicted_emotion = emotion_classes[emotion_label]\n",
    "\n",
    "# Print the custom test image and its predicted label\n",
    "print(f\"Predicted Emotion: {predicted_emotion}\")\n",
    "print(f\"Confidence [happy, sad, angry]: {prediction_prob}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Display the custom test image using matplotlib\n",
    "plt.imshow(custom_test_image[0, :, :, 0])\n",
    "plt.title(f\"Predicted Emotion: {predicted_emotion}\")\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()\n",
    "\n",
    "from PIL import Image\n",
    "# Display the original custom test image using PIL\n",
    "img_pil = Image.open(custom_test_image_path)\n",
    "plt.imshow(np.array(img_pil))\n",
    "plt.title(f\"Predicted Emotion: {predicted_emotion}\")\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "* Feature Extraction: This approach uses very basic feature extraction (flattening the image), which might not capture the necessary details for accurate emotion classification.\n",
    "* Model Complexity: Logistic regression is quite basic for image classification tasks.\n",
    "* Data Quality: The quality and size of your dataset will significantly impact the performance of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
